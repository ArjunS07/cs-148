{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST with MLP (PyTorch)\n",
    "\n",
    "In this example you'll train a Multi Layer Perceptron (MLP) on the Fashion MNIST.\n",
    "An MLP is known as a \"fully connected neural network\" because every perceptron/neuron\n",
    "is connected to every other perceptron in the next layer.\n",
    "\n",
    "This is a widely used dataset, kind of like the *Hello World!* of deep learning.\n",
    "The dataset contains **60k training samples** and **10k testing samples** where\n",
    "each sample is a small **$28 \\times 28$** grayscale image. Each sample also has\n",
    "a label, which corresponds to the **class** or type of clothing shown in the\n",
    "image.\n",
    "\n",
    "Here are the classes and their integer labels:\n",
    "\n",
    "| **Label** | **Description**  |\n",
    "|-----------|------------------|\n",
    "| 0         | T-shirt/top      |\n",
    "| 1         | Trouser          |\n",
    "| 2         | Pullover         |\n",
    "| 3         | Dress            |\n",
    "| 4         | Coat             |\n",
    "| 5         | Sandal           |\n",
    "| 6         | Shirt            |\n",
    "| 7         | Sneaker          |\n",
    "| 8         | Bag              |\n",
    "| 9         | Ankle boot       |\n",
    "\n",
    "Take a look at some of the samples using the dataset viewer: https://huggingface.co/datasets/zalando-datasets/fashion_mnist/viewer/fashion_mnist/train\n",
    "\n",
    "Here are some examples (three rows of samples for each class):\n",
    "\n",
    "![dataset preview](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import zipfile\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "We will...\n",
    "- download the `fashion_mnist.zip` from google drive\n",
    "- unzip it into the `../dataset` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1EQPgZ1401yUN0Y64YNcAIKWMuxgA4ID4\n",
      "From (redirected): https://drive.google.com/uc?id=1EQPgZ1401yUN0Y64YNcAIKWMuxgA4ID4&confirm=t&uuid=8a40749b-21b6-433a-a653-cbaf4fe32bde\n",
      "To: /Users/armeetjatyani/Documents/caltech/ta/cs150-ps1/fashion_mnist.zip\n",
      "100%|██████████| 48.6M/48.6M [00:07<00:00, 6.73MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../fashion_mnist.zip'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download fashion_mnist.zip file from google drive\n",
    "\n",
    "file_id = \"1EQPgZ1401yUN0Y64YNcAIKWMuxgA4ID4\"\n",
    "file_url = f\"https://drive.google.com/file/d/{file_id}\"  # for your reference if you want to manually download\n",
    "gdown.download(id=file_id, output=\"../fashion_mnist.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete. Take a look inside the dataset/ folder before you move on!\n"
     ]
    }
   ],
   "source": [
    "# unzip the file\n",
    "\n",
    "zip_path = '../fashion_mnist.zip'\n",
    "extract_to = '../'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "print(\"Extraction complete. Take a look inside the dataset/ folder before you move on!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "In the PyTorch workflow, we always make a custom dataset that extends `torch.utils.data.Dataset`.\n",
    "Every `Dataset` must implement the following methods...\n",
    "- `__init__(self, ...)`: load dataset metadata and sample filepaths\n",
    "- `__getitem__(self, idx)`: returns dataset sample at index: idx, we load the image from the filepath, normalize it, and also load it's label (one-hot encoded vector to represent class)\n",
    "- `__len__(self)` methods: returns length of the dataset (how many samples there are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"MNIST Fashion Dataset\n",
    "    Each sample is a tuple (x, y)\n",
    "    - x is a 28x28 grayscale image (normalized to pixel values from 0-1)\n",
    "    - y is a one-hot encoded vector representing the class of this sample\n",
    "        Read about one-hot encoding here: https://www.youtube.com/watch?v=G2iVj7WKDFk\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.meta = pd.read_csv(csv_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, label = self.meta.iloc[idx]\n",
    "\n",
    "        # ------------------------------------------------------ x: image tensor\n",
    "        image = Image.open(\"../\" + filepath)\n",
    "        # Define a transform to convert PIL image -> torch.Tensor\n",
    "        transform = transforms.Compose([transforms.PILToTensor()])\n",
    "        img_tensor = transform(image)[0] # there is only 1 channel (black and white)\n",
    "        img_tensor = img_tensor.float() / 255.0 # normalize all pixel between 0 and 1\n",
    "\n",
    "        # ------------------------------------------------------ y: label tensor\n",
    "        NUM_CLASSES = 10\n",
    "        label_tensor = torch.zeros((NUM_CLASSES,)).float()\n",
    "        label_tensor[label - 1] = 1.0\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of datasets: 60000 train + 10000 test\n",
      "Explore sample:\n",
      "╭───────────────┬──────────────────────┬───────────────┬──────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Description   │ Shape                │ DType         │ Preview                                                                          │\n",
      "├───────────────┼──────────────────────┼───────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Input Image   │ torch.Size([28, 28]) │ torch.float32 │ tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000, 0.0000, │\n",
      "│               │                      │               │          0.0000, 0.0000, 0.2275, 0.3451, 0.4000, 0.3059, 0.3922, 0.5020, 0.3686, │\n",
      "│               │                      │               │          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, │\n",
      "│               │                      │               │          0.0000],                                                                │\n",
      "│               │                      │               │         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0078, 0.0000, │\n",
      "│               │                      │               │          0.0000, 0.0353, 0.8314, 0.6706, 0.7647, 0.6902, 0.8706, 0.7373, 0.7294, │\n",
      "│               │                      │               │          0.2314, 0.0000, 0.0039, 0.0039, 0.0118, 0.0039, 0.0000, 0.0000, 0.00... │\n",
      "├───────────────┼──────────────────────┼───────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output Label  │ torch.Size([10])     │ torch.float32 │ tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])                                 │\n",
      "╰───────────────┴──────────────────────┴───────────────┴──────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load datasets\n",
    "\n",
    "partition semantics\n",
    "-------------------\n",
    "* all partitions are disjoint (no overlapping samples)\n",
    "train - used to train your samples\n",
    "validation - used to validate your samples (during training)\n",
    "test - used to test your model (after training)\n",
    "\n",
    "NOTE: the point of having separate validation and test sets is to avoid over-\n",
    "correcting your hyperparameters to the test set. You can monitor your model's\n",
    "performance during training using the validation set (which the model never \n",
    "sees in training) and then do your final test using the testing set.\n",
    "\n",
    "For simplicity, we'll make our validation and test set the same.\n",
    "\"\"\"\n",
    "train_dataset = FashionDataset(\"../dataset/train.csv\")\n",
    "test_dataset = FashionDataset(\"../dataset/test.csv\")\n",
    "\n",
    "print(f\"Lengths of datasets: {len(train_dataset)} train + {len(test_dataset)} test\")\n",
    "\n",
    "# get a sample and explore it\n",
    "index = 32\n",
    "image, label = train_dataset[index]\n",
    "data = [\n",
    "    [\"Input Image\", image.shape, image.dtype, str(image)[:500] + \"...\"],\n",
    "    [\"Output Label\", label.shape, image.dtype, label]\n",
    "]\n",
    "print(\"Explore sample:\")\n",
    "print(tabulate(data, headers=[\"Description\", \"Shape\", \"DType\", \"Preview\"], tablefmt=\"rounded_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Dataloaders allow you to load your Dataset samples in *batches* (multiple \n",
    "samples at a time). Batch sizes can affect training dynamics. A rough rule to follow:\n",
    "- lower batch sizes give more accurate gradient estimates, which can help the model converge better but may result in slower training.\n",
    "- higher batch sizes speed up training by using hardware more efficiently, but they may lead to less accurate gradient estimates and poorer generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore sample batch:\n",
      "╭─────────────────────────┬──────────────────────────┬───────────────┬──────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Description             │ Shape                    │ DType         │ Preview                                                                          │\n",
      "├─────────────────────────┼──────────────────────────┼───────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Input Images (batched)  │ torch.Size([64, 28, 28]) │ torch.float32 │ tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000, 0.0000, │\n",
      "│                         │                          │               │          0.0000, 0.0000, 0.2275, 0.3451, 0.4000, 0.3059, 0.3922, 0.5020, 0.3686, │\n",
      "│                         │                          │               │          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, │\n",
      "│                         │                          │               │          0.0000],                                                                │\n",
      "│                         │                          │               │         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0039, 0.0078, 0.0000, │\n",
      "│                         │                          │               │          0.0000, 0.0353, 0.8314, 0.6706, 0.7647, 0.6902, 0.8706, 0.7373, 0.7294, │\n",
      "│                         │                          │               │          0.2314, 0.0000, 0.0039, 0.0039, 0.0118, 0.0039, 0.0000, 0.0000, 0.00... │\n",
      "├─────────────────────────┼──────────────────────────┼───────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output Labels (batched) │ torch.Size([64, 10])     │ torch.float32 │ tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],                                │\n",
      "│                         │                          │               │         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],                                │\n",
      "│                         │                          │               │ ...                                                                              │\n",
      "╰─────────────────────────┴──────────────────────────┴───────────────┴──────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.01,\n",
    "    \"num_epochs\": 50,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyper_params[\"batch_size\"], shuffle=True)\n",
    "val_loader  = DataLoader(test_dataset, batch_size=hyper_params[\"batch_size\"]) # no need to shuffle\n",
    "\n",
    "# get a single \"batch\" and explore it\n",
    "for images, labels in train_loader:\n",
    "    data = [\n",
    "        [\"Input Images (batched)\", images.shape, images.dtype, str(image)[:500] + \"...\"],\n",
    "        [\"Output Labels (batched)\", labels.shape, labels.dtype, str(labels)[:100] + \"...\"]\n",
    "    ]\n",
    "    print(\"Explore sample batch:\")\n",
    "    print(tabulate(data, headers=[\"Description\", \"Shape\", \"DType\", \"Preview\"], tablefmt=\"rounded_grid\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n",
    "\n",
    "We've provided some boilerplate. Your job is to choose your optimizer and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            # TODO: (3.d) define your layers here\n",
    "            # you can use any activation function, normalization methods, and/or dropout\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.optim = None # TODO: (3.d) choose an optimizer (do some research)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"train\": {\n",
    "                \"losses\": [],\n",
    "                \"accuracies\": [],\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"losses\": [],\n",
    "                \"accuracies\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.mlp(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train method\n",
    "-----------\n",
    "- gets called once every training \"epoch\"\n",
    "- An epoch refers to a complete pass through the entire training dataset. \n",
    "- During an epoch, the model processes batches of data sequentially, computes gradients, and updates parameters.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(model, dataloader):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_correct = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for image_batch, label_batch in dataloader:\n",
    "        # zero out gradients from previous batch\n",
    "        model.optim.zero_grad()\n",
    "\n",
    "        # output ~ predicted labels (batched)\n",
    "        output = model(image_batch)\n",
    "        loss: torch.Tensor = None  # TODO: (3.d) choose a loss function\n",
    "        epoch_loss += float(loss.data) / dataloader.batch_size\n",
    "\n",
    "        # calculates all gradients (like the .backward we implemented)\n",
    "        loss.backward()\n",
    "        # tells the optimizer to perform an update \"step\" on all model parameters\n",
    "        model.optim.step()\n",
    "\n",
    "        # check if model predictions were correct and increment epoch_correct\n",
    "        predicted_labels = None  # TODO: (3.d)\n",
    "        actual_labels = None  # TODO: (3.d)\n",
    "        iteration_correct = -1  # TODO: (3.d)\n",
    "        epoch_correct += iteration_correct\n",
    "\n",
    "    # model.metrics is a dictionary (see the model class below)\n",
    "    model.metrics[\"train\"][\"losses\"].append(epoch_loss / len(dataloader.dataset))\n",
    "    model.metrics[\"train\"][\"accuracies\"].append(\n",
    "        100 * epoch_correct / len(dataloader.dataset)\n",
    "    )\n",
    "\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    # set model to evaluation mode (passes don't change stored gradients)\n",
    "    # when evaluating our model, we should never update our parameters\n",
    "    model.eval()\n",
    "\n",
    "    epoch_correct = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # disables any gradient tracking explicitly\n",
    "    with torch.no_grad():\n",
    "        for image_batch, label_batch in dataloader:\n",
    "            # zero out gradients from previous batch\n",
    "            model.optim.zero_grad()\n",
    "\n",
    "            # output ~ predicted labels (batched)\n",
    "            output = model(image_batch)\n",
    "            loss: torch.Tensor = None  # TODO: (3.d) choose a loss function\n",
    "            epoch_loss += float(loss.data) / dataloader.batch_size\n",
    "\n",
    "            # check if model predictions were correct and increment epoch_correct\n",
    "            predicted_labels = None  # TODO: (3.d)\n",
    "            actual_labels = None  # TODO: (3.d)\n",
    "            iteration_correct = -1  # TODO: (3.d)\n",
    "            epoch_correct += iteration_correct\n",
    "\n",
    "    # model.metrics is a dictionary (see the model class below)\n",
    "    model.metrics[\"val\"][\"losses\"].append(epoch_loss / len(dataloader.dataset))\n",
    "    model.metrics[\"val\"][\"accuracies\"].append(\n",
    "        100 * epoch_correct / len(dataloader.dataset)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models!\n",
    "\n",
    "Now that you've setup your datasets, dataloaders, model, training and \n",
    "validation steps, and chosen your optimizer loss criterion, it's time to actually\n",
    "train for a number of epochs. \n",
    "\n",
    "Since you saved your training and validation accuracies and losses, we can also \n",
    "generate some useful plots to analyze how well your model is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
